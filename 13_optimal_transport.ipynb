{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "import ot\n",
    "import os\n",
    "# import sys\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import List\n",
    "\n",
    "# import collections\n",
    "\n",
    "from ete3 import Tree\n",
    "from skbio import TreeNode\n",
    "import Levenshtein\n",
    "from Bio import SeqIO, Align\n",
    "from skbio.diversity.beta import weighted_unifrac, unweighted_unifrac\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Classifiers to test\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "from src import fasta_to_kmer_vector\n",
    "\n",
    "# Number of folds for K-fold validation\n",
    "N_FOLDS = 10\n",
    "\n",
    "# Which BLAST database to look for OTU --> genome mappings\n",
    "GENOME_DATABASE = \"core_nt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join(\"data\")\n",
    "SAMPLE_DFS_DIR = os.path.join(\"data\", \"samples_by_samples_dataframes\")\n",
    "OT_COST_MATRICES_DIR = os.path.join(\"data\", \"ot_cost_matrices\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(SAMPLE_DFS_DIR, exist_ok=True)\n",
    "os.makedirs(OT_COST_MATRICES_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "# Download the annotated GreenGenes tree if it's not already in data/\n",
    "if [ ! -f \"data/gg_13_5_otus_99_annotated.tree\" ]; then\n",
    "    echo \"Downloading accession information from GreenGenes\"\n",
    "    wget --no-check-certificate https://ftp.microbio.me/greengenes_release/gg_13_5/gg_13_5_otus_99_annotated.tree.gz -O ${ROOT}/data/gg_13_5_otus_99_annotated.tree.gz\n",
    "    gzip -v -d ${ROOT}/data/gg_13_5_accessions.txt.gz\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Tree(\"data/gg_13_5_otus_99_annotated.tree\", format=1, quoted_node_names=True)\n",
    "skbio_tree = TreeNode.read(StringIO(tree.write(format_root_node=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 16S table, get top N OTUs\n",
    "# TODO: Use columns other than `sample`\n",
    "ibd_data = pd.read_csv(\"ihmp/ibd_data.csv.gz\", dtype={0: str}, compression='gzip')\n",
    "_otus = ibd_data.set_index(ibd_data.columns[3])\n",
    "_otus.drop(columns=['patient', 'visit', 'site'], inplace=True)\n",
    "otus = _otus.T.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We care about columns:\n",
    "# dianosis: CD = chrons disease, UC = ulcerative colitis, nonIBD = control\n",
    "ibd_metadata = pd.read_csv(\"ihmp/ibd_metadata_new.csv\")\n",
    "\n",
    "# Drop the nonIBD label since we only have 1 example of it\n",
    "non_ibd_index = ibd_metadata[ibd_metadata.diagnosis == \"nonIBD\"].index.item()\n",
    "non_ibd_sample_id = ibd_metadata[ibd_metadata.diagnosis == \"nonIBD\"]['sample'].item()\n",
    "ibd_metadata.drop(index=non_ibd_index, inplace=True)\n",
    "\n",
    "otus.drop(columns=non_ibd_sample_id, inplace=True)\n",
    "\n",
    "# Ignore everything except sample id and diagnosis\n",
    "ibd_metadata_diagnosis = ibd_metadata[['sample', 'diagnosis']].set_index('sample').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that metadata and normal samples have same sample ids\n",
    "num_samples_data = otus.shape[1]\n",
    "sample_intersection = len(set(ibd_metadata['sample'].tolist()) & set(ibd_data['sample'].tolist()))\n",
    "\n",
    "assert num_samples_data == sample_intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_names = otus.index\n",
    "otus_subset = otus.loc[otu_names].copy()\n",
    "\n",
    "gg = SeqIO.index(\"data/gg_13_5.fasta\", \"fasta\")\n",
    "\n",
    "# Get sequences for top N_SEQS OTUs\n",
    "topN_seqs = [gg[otu] for otu in otu_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skbio_subtree = skbio_tree.shear(otu_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating OT Cost Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16S Sequence Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_cost_path = os.path.join(OT_COST_MATRICES_DIR, \"levenshtein_cost_matrix.npy\")\n",
    "\n",
    "if os.path.exists(levenshtein_cost_path):\n",
    "    print(\"Loading Levenshtein distance based cost matrix from disk\")\n",
    "    \n",
    "    with open(levenshtein_cost_path, \"rb\") as file_in:\n",
    "        levenshtein_cost_matrix = np.load(file_in)\n",
    "else:\n",
    "    print(\"Calculating Levenshtein distance based cost matrix\")\n",
    "    levenshtein_cost_matrix = np.zeros((len(topN_seqs), len(topN_seqs)), dtype=int)\n",
    "    \n",
    "    for i in range(len(topN_seqs)):\n",
    "        for j in range(i + 1, len(topN_seqs)):\n",
    "            dist = Levenshtein.distance(str(topN_seqs[i].seq), str(topN_seqs[j].seq))\n",
    "            levenshtein_cost_matrix[i, j] = levenshtein_cost_matrix[j, i] = dist\n",
    "    \n",
    "    with open(levenshtein_cost_path, \"wb\") as f:\n",
    "        np.save(f, levenshtein_cost_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alignment-based Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_cost_path = os.path.join(OT_COST_MATRICES_DIR, \"alignment_cost_matrix.npy\")\n",
    "\n",
    "if os.path.exists(alignment_cost_path):\n",
    "    print(\"Loading alignment cost matrix from disk\")\n",
    "    \n",
    "    with open(alignment_cost_path, \"rb\") as file_in:\n",
    "        alignment_cost_matrix = np.load(file_in)\n",
    "else:\n",
    "    print(\"Calculating alignment cost matrix\")\n",
    "    alignment_cost_matrix = np.zeros((len(topN_seqs), len(topN_seqs)), dtype=int)\n",
    "    aligner = Align.PairwiseAligner()\n",
    "\n",
    "    for i in range(len(topN_seqs)):\n",
    "        for j in range(i + 1, len(topN_seqs)):\n",
    "            dist = aligner.align(topN_seqs[i].seq, topN_seqs[j].seq).score\n",
    "            alignment_cost_matrix[i, j] = dist\n",
    "            \n",
    "    with open(alignment_cost_path, \"wb\") as file_out:\n",
    "        np.save(file_out, alignment_cost_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phylogenetic Distance (Sum of GreenGene Edge Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogenetic_distance_cost_path = os.path.join(OT_COST_MATRICES_DIR, \"phylogenetic_cost_matrix.npy\")\n",
    "\n",
    "if os.path.exists(phylogenetic_distance_cost_path):\n",
    "    print(\"Loading phylogenetic distance based cost matrix from disk\")\n",
    "    \n",
    "    with open(phylogenetic_distance_cost_path, \"rb\") as file_in:\n",
    "        phylogenetic_cost_matrix = np.load(file_in)\n",
    "else:\n",
    "    print(\"Calculating phylogenetic distance based cost matrix\")\n",
    "    \n",
    "    skbio_subtree = skbio_tree.shear(set(otus.index.to_list()))\n",
    "    phylogenetic_cost_matrix = skbio_subtree.tip_tip_distances().data\n",
    "    \n",
    "    with open(phylogenetic_distance_cost_path, \"wb\") as file_out:\n",
    "        np.save(file_out, phylogenetic_cost_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome Based Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# Get the root of the git repo\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "mkdir -p ${ROOT}/data/otu_fastas\n",
    "\n",
    "# If there's no otus.txt file, extract otus from ibd_data.csv and put it in otus.txt\n",
    "if [ ! -f \"${ROOT}/data/otus.txt\" ]; then\n",
    "    echo \"Extracting OTUs from Samples x OTUs table\"\n",
    "    head -n 1 ihmp/ibd_data.csv | tr ',' '\\n' | tail -n +5 > ${ROOT}/data/otus.txt\n",
    "fi\n",
    "\n",
    "# Extract sequence information from the master fasta file into fasta file per OTU \n",
    "while read -r otu; do \n",
    "    grep \"^>${otu}$\" -A 1 data/gg_13_5.fasta > ${ROOT}/data/otu_fastas/${otu}.fasta; \n",
    "    echo $otu; \n",
    "done < ${ROOT}/data/otus.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s {GENOME_DATABASE}\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# Grab GENOME_DATABSE which is declared in the top-most cell\n",
    "GENOME_DATABASE=$1\n",
    "\n",
    "# Get the root of the git repo\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "mkdir -p ${ROOT}/data/blast/${GENOME_DATABASE}\n",
    "\n",
    "while read -r otu; do\n",
    "    if [ ! -f ${ROOT}/data/blast/${GENOME_DATABASE}/${otu}.zip ] && [ ! -d ${ROOT}/data/blast/${GENOME_DATABASE}/${otu} ]; then\n",
    "        python ${ROOT}/src/search_blast.py \\\n",
    "            -d ${GENOME_DATABASE} \\\n",
    "            -o ${ROOT}/data/blast/${GENOME_DATABASE}/${otu}.zip \\\n",
    "            -t 300 \\\n",
    "            ${ROOT}/data/otu_fastas/${otu}.fasta\n",
    "    else\n",
    "        echo \"Skipping ${otu} because output exists\";\n",
    "    fi\n",
    "done < ${ROOT}/data/otus.txt\n",
    "\n",
    "\n",
    "for file in ${ROOT}/data/blast/${GENOME_DATABASE}/*.zip; do\n",
    "    unzip -d ${file%.zip} $file\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_search_dir = os.path.join(f\"data/blast/{GENOME_DATABASE}\")\n",
    "\n",
    "# For every OTU, list all taxids corresponding to a complete genome\n",
    "otu_to_taxid = defaultdict(set)\n",
    "pbar = tqdm(os.listdir(blast_search_dir), total=len(os.listdir(blast_search_dir)))\n",
    "\n",
    "for otu in pbar:\n",
    "    subdir = os.path.join(blast_search_dir, otu)\n",
    "    if os.path.isdir(subdir):\n",
    "        files = os.listdir(subdir)\n",
    "        info = [f for f in files if f.endswith(\"_1.json\")][0]\n",
    "        info_path = os.path.join(subdir, info)\n",
    "        \n",
    "        with open(info_path, \"r\") as f:\n",
    "            info_dict = json.load(f)\n",
    "            \n",
    "        hits = info_dict['BlastOutput2']['report']['results']['search']['hits']\n",
    "        for hit in hits:\n",
    "            if 'title' in hit['description'][0].keys():\n",
    "                if \"complete genome\" in hit['description'][0]['title']:\n",
    "                    otu_to_taxid[otu].add(hit['description'][0]['taxid'])\n",
    "\n",
    "# Convert sets to lists\n",
    "otu_to_taxid_first_hit = {k: list(v)[0] for k,v in otu_to_taxid.items()}\n",
    "unique_taxids = list(set(otu_to_taxid_first_hit.values()))\n",
    "\n",
    "with open(\"data/unique_taxids.txt\", \"w\") as file_out:\n",
    "    for taxid in unique_taxids:\n",
    "        file_out.write(str(taxid) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s {GENOME_DATABASE}\n",
    "\n",
    "# Activate the Conda environment\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "conda activate base  # Replace \"base\" with your desired Conda environment name\n",
    "\n",
    "echo \"Using Conda environment: $(conda env list | grep '*' | awk '{print $1}')\"\n",
    "\n",
    "# Check if ncbi-datasets-cli is installed in the base environment\n",
    "if ! command -v datasets &> /dev/null; then\n",
    "    echo \"Error: 'datasets' command is not installed. Please install it before running this script.\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "GENOME_DATABASE=$1\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "echo \"Downloading genomes\"\n",
    "mkdir -p ${ROOT}/data/genomes/${GENOME_DATABASE}\n",
    "\n",
    "while read -r line; do \n",
    "    OUTFILE=\"data/genomes/${GENOME_DATABASE}/${line}.zip\"\n",
    "    OUTDIR=\"data/genomes/${GENOME_DATABASE}/${line}\"\n",
    "\n",
    "    if [ ! -d \"${OUTDIR}\" ] && [ ! -f \"${OUTFILE}\" ]; then\n",
    "        echo ${line}\n",
    "        datasets download genome taxon \"${line}\" \\\n",
    "            --fast-zip-validation \\\n",
    "            --reference \\\n",
    "            --assembly-version latest \\\n",
    "            --assembly-level complete \\\n",
    "            --exclude-atypical \\\n",
    "            --no-progressbar \\\n",
    "            --filename \"${OUTFILE}\"; \n",
    "    else\n",
    "        echo \"Skipping ${line} since ${OUTFILE} or ${OUTDIR} already exists\"\n",
    "    fi\n",
    "done < ${ROOT}/data/unique_taxids.txt\n",
    "\n",
    "for file in data/genomes/${GENOME_DATABSE}/*.zip; do\n",
    "    if [ ! -d ${file%.zip} ]; then\n",
    "        unzip -d ${file%.zip} $file\n",
    "        rm -f $file\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s {GENOME_DATABASE}\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "GENOME_DATABASE=$1\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "for taxid in ${ROOT}/data/genomes/${GENOME_DATABASE}/*; do\n",
    "    id_only=$(basename ${taxid})\n",
    "    source_directory=\"${ROOT}/data/genomes/${GENOME_DATABASE}/${id_only}/ncbi_dataset/data/\"\n",
    "    target_directory=\"${ROOT}/data/genomes/${GENOME_DATABASE}/${id_only}/\"\n",
    "    \n",
    "    # Find all .fna files recursively and copy them to the taxid directory\n",
    "    find ${source_directory} -type f -name \"*.fna\" -exec cp -v {}/*.fna \"${target_directory}\" \\;\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s {GENOME_DATABASE}\n",
    "\n",
    "# NOTE: if you have GNU parallel installed, run this in the terminal to get the same output much faster (from within phylosig):\n",
    "# parallel python src/fasta_to_kmer_vector.py -i {} -o data/genomes/core_nt/{/.}.kmer_vector.tsv -v ::: $(find data/genomes -type f -name \"*.fna\")\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "GENOME_DATABASE=$1\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "for input_file in $(find ${ROOT}/data/genomes/${GENOME_DATABASE} -maxdepth 2 -type f -name \"*.fna\"); do\n",
    "    output_file=\"${ROOT}/data/genomes/${GENOME_DATABASE}/$(basename \"$input_file\" .fna).kmer_vector.tsv\"\n",
    "    \n",
    "    if [[ ! -f \"$output_file\" ]]; then\n",
    "        echo \"${input_file} -> ${output_file}\"\n",
    "        python src/fasta_to_kmer_vector.py -i \"${input_file}\" -o \"${output_file}\" -v\n",
    "    else\n",
    "        echo \"${output_file} already exists. Skipping.\"\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s {GENOME_DATABASE}\n",
    "\n",
    "GENOME_DATABASE=$1\n",
    "ROOT=$(git rev-parse --show-toplevel)\n",
    "\n",
    "# NOTE: This is simply assigning the lexicographically first kmer vector.\n",
    "# There are probably smarter ways to collate this (e.g. take median of values in each k-mer bin)\n",
    "for taxid in ${ROOT}/data/genomes/${GENOME_DATABASE}/*; do \n",
    "    id_only=$(basename $taxid)\n",
    "    echo ${id_only} $(ls ${taxid}/*.kmer_vector.tsv | head -n 1); \n",
    "done > ${ROOT}/data/_taxid_and_kmer.txt\n",
    "\n",
    "cat ${ROOT}/data/_taxid_and_kmer.txt | tr ' ' '\\t' > ${ROOT}/data/taxid_and_kmer.txt\n",
    "rm -f ${ROOT}/data/_taxid_and_kmer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/taxid_and_kmer.txt\", sep=\"\\t\", names=[\"taxid\", \"kmer_vector_path\"]).drop_duplicates(subset=\"taxid\")\n",
    "taxid_to_otu = {v:k for k,v in otu_to_taxid_first_hit.items()}\n",
    "df['otu'] = df['taxid'].map(taxid_to_otu)\n",
    "\n",
    "df.drop('taxid', axis=1, inplace=True)\n",
    "df.set_index('otu', inplace=True)\n",
    "\n",
    "otu_to_vector = {\n",
    "    otu: pd.read_csv(vector_path, sep=\"\\t\", index_col=0).to_numpy().flatten()\n",
    "    for otu, vector_path in df['kmer_vector_path'].to_dict().items()\n",
    "}\n",
    "\n",
    "index = pd.read_csv(df.iloc[0].item(), sep=\"\\t\", index_col=0).index.to_list()\n",
    "\n",
    "genome_df = pd.DataFrame(otu_to_vector, index=index)\n",
    "\n",
    "# Ensure that genome_df's columns are in the same order as otus index\n",
    "genome_df_columns = [col for col in otus.index if col in genome_df.columns]\n",
    "genome_df = genome_df[genome_df_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_cost_matrix = np.zeros((len(genome_df_columns), len(genome_df_columns)), dtype=float)\n",
    "genome_normalized_cost_matrix = np.zeros((len(genome_df_columns), len(genome_df_columns)), dtype=float)\n",
    "\n",
    "for i in range(len(genome_df_columns)):\n",
    "    for j in range(len(genome_df_columns)):\n",
    "        otu_i = genome_df[genome_df_columns[i]].to_numpy()\n",
    "        otu_j = genome_df[genome_df_columns[j]].to_numpy()\n",
    "        \n",
    "        otu_i_norm = otu_i / np.sum(otu_i)\n",
    "        otu_j_norm = otu_j / np.sum(otu_j)\n",
    "        \n",
    "        dist = np.linalg.norm(otu_i - otu_j, 2)\n",
    "        norm_dist = np.linalg.norm(otu_i_norm - otu_j_norm, 2)\n",
    "        \n",
    "        genome_cost_matrix[i, j] = dist\n",
    "        genome_normalized_cost_matrix[i, j] = norm_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Sample x Sample Cost Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16S Sequence Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "levenshtein_sample_df_path = os.path.join(SAMPLE_DFS_DIR, \"levenshtein_sample_cost_df.csv\")\n",
    "\n",
    "if os.path.exists(levenshtein_sample_df_path):\n",
    "    levenshtein_sample_cost_df = pd.read_csv(levenshtein_sample_df_path, index_col=0)\n",
    "else:\n",
    "    num_samples = otus.shape[1]\n",
    "    levenshtein_sample_cost_matrix = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    # Calculate the upper triangle of (samples x samples) total cost matrix\n",
    "    # Each entry is sum of entries in the transport matrix generated by OT\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            # Create normalized p_i and q_j vectors\n",
    "            p = otus[otus.columns[i]].to_numpy()\n",
    "            p_norm = p / p.sum()\n",
    "            q = otus[otus.columns[j]].to_numpy()\n",
    "            q_norm = q / q.sum()\n",
    "            \n",
    "            # Calculate total cost which is equivalent to the Wasserstein distance\n",
    "            total_cost = ot.emd2(p, q, levenshtein_cost_matrix, log=False, check_marginals=False)\n",
    "            \n",
    "            # Assign the cost to the corresponding entry\n",
    "            levenshtein_sample_cost_matrix[i, j] = total_cost\n",
    "                \n",
    "    levenshtein_sample_cost_df = pd.DataFrame(levenshtein_sample_cost_matrix, index=otus.columns, columns=otus.columns)\n",
    "    levenshtein_sample_cost_df.to_csv(levenshtein_sample_df_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_sample_df_path = os.path.join(SAMPLE_DFS_DIR, \"alignment_sample_cost_df.csv\")\n",
    "\n",
    "if os.path.exists(alignment_sample_df_path):\n",
    "    alignment_sample_cost_df = pd.read_csv(alignment_sample_df_path, index_col=0)\n",
    "else:\n",
    "    num_samples = otus.shape[1]\n",
    "    alignment_sample_cost_matrix = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    # Calculate the upper triangle of (samples x samples) total cost matrix\n",
    "    # Each entry is sum of entries in the transport matrix generated by OT\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            # Create normalized p_i and q_j vectors\n",
    "            p = otus[otus.columns[i]].to_numpy()\n",
    "            p_norm = p / p.sum()\n",
    "            q = otus[otus.columns[j]].to_numpy()\n",
    "            q_norm = q / q.sum()\n",
    "            \n",
    "            # Calculate total cost which is equivalent to the Wasserstein distance\n",
    "            total_cost = ot.emd2(p, q, alignment_cost_matrix, log=False, check_marginals=False)\n",
    "            \n",
    "            # Assign the cost to the corresponding entry\n",
    "            alignment_sample_cost_matrix[i, j] = total_cost\n",
    "                \n",
    "\n",
    "    alignment_sample_cost_df = pd.DataFrame(alignment_sample_cost_matrix, index=otus.columns, columns=otus.columns)\n",
    "    alignment_sample_cost_df.to_csv(alignment_sample_df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phylogenetic Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogenetic_sample_cost_path = os.path.join(SAMPLE_DFS_DIR, \"phylogenetic_sample_cost_df.csv\")\n",
    "\n",
    "if os.path.exists(phylogenetic_sample_cost_path):\n",
    "    phylogenetic_sample_cost_df = pd.read_csv(phylogenetic_sample_cost_path, index_col=0)\n",
    "else:\n",
    "    num_samples = otus.shape[1]\n",
    "    phylogenetic_sample_cost_matrix = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    # Calculate the upper triangle of (samples x samples) total cost matrix\n",
    "    # Each entry is sum of entries in the transport matrix generated by OT\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            # Create normalized p_i and q_j vectors\n",
    "            p = otus[otus.columns[i]].to_numpy()\n",
    "            p_norm = p / p.sum()\n",
    "            q = otus[otus.columns[j]].to_numpy()\n",
    "            q_norm = q / q.sum()\n",
    "            \n",
    "            # Calculate total cost which is equivalent to the Wasserstein distance\n",
    "            total_cost = ot.emd2(p, q, phylogenetic_cost_matrix, log=False, check_marginals=False)\n",
    "            \n",
    "            # Assign the cost to the corresponding entry\n",
    "            phylogenetic_sample_cost_matrix[i, j] = total_cost\n",
    "            \n",
    "    phylogenetic_sample_cost_df = pd.DataFrame(phylogenetic_sample_cost_matrix, index=otus.columns, columns=otus.columns)\n",
    "    phylogenetic_sample_cost_df.to_csv(phylogenetic_sample_cost_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genome Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_normalized_sample_cost_path = os.path.join(SAMPLE_DFS_DIR, \"genome_normalized_sample_cost_df.csv\")\n",
    "genome_sample_cost_path = os.path.join(SAMPLE_DFS_DIR, \"genome_sample_cost_df.csv\")\n",
    "\n",
    "\n",
    "if os.path.exists(genome_sample_cost_path) and os.path.exists(genome_normalized_sample_cost_path):\n",
    "    genome_sample_cost_df = pd.read_csv(genome_sample_cost_path, index_col=0)\n",
    "    genome_normalized_sample_cost_df = pd.read_csv(genome_normalized_sample_cost_path, index_col=0)    \n",
    "else:\n",
    "    # Some pandas shenanigans to get only columns where the sum of the \n",
    "    # subset of OTUs from the genome dataframe sum to > 0\n",
    "    # This is to prevent infeasible problems in the OT formulation\n",
    "    samples = otus.T.loc[otus.loc[genome_df_columns].sum(axis=0) != 0].T.columns.to_list()\n",
    "\n",
    "    num_samples = len(samples)\n",
    "    genome_sample_cost_matrix = np.zeros((num_samples, num_samples))\n",
    "    genome_normalized_sample_cost_matrix = np.zeros((num_samples, num_samples))\n",
    "\n",
    "    # Calculate the upper triangle of (samples x samples) total cost matrix\n",
    "    # Each entry is sum of entries in the transport matrix generated by OT\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_samples):\n",
    "            # Create normalized p_i and q_j vectors\n",
    "            p = otus[samples[i]][genome_df_columns].to_numpy()\n",
    "            q = otus[samples[j]][genome_df_columns].to_numpy()\n",
    "            \n",
    "            # Sometimes, subsetting p and q to only the OTUs for the genomes we have results in a 0 vector\n",
    "            # When this is the case, the OT problem is infeasible\n",
    "            if (p.sum().item() != 0) and (q.sum().item() != 0):\n",
    "                p_norm = p / p.sum()\n",
    "                q_norm = q / q.sum()\n",
    "                total_cost = ot.emd2(p, q, genome_cost_matrix, log=False, check_marginals=False)\n",
    "                total_cost_normalized = ot.emd2(p, q, genome_normalized_cost_matrix, log=False, check_marginals=False)\n",
    "            else:\n",
    "                total_cost_normalized = genome_normalized_cost_matrix.max(axis=1).sum()\n",
    "                total_cost = genome_cost_matrix.max(axis=1).sum()\n",
    "            \n",
    "            # Assign the cost to the corresponding entry\n",
    "            genome_normalized_sample_cost_matrix[i, j] = total_cost_normalized\n",
    "            genome_sample_cost_matrix[i, j] = total_cost\n",
    "                \n",
    "    genome_sample_cost_df = pd.DataFrame(genome_sample_cost_matrix, index=samples, columns=samples)\n",
    "    genome_normalized_sample_cost_df = pd.DataFrame(genome_normalized_sample_cost_matrix, index=samples, columns=samples)\n",
    "    \n",
    "    genome_sample_cost_df.to_csv(genome_sample_cost_path)\n",
    "    genome_normalized_sample_cost_df.to_csv(genome_normalized_sample_cost_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UniFrac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unweighted UniFrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_unifrac_sample_cost_path = os.path.join(SAMPLE_DFS_DIR, \"unweighted_unifrac_sample_cost_df.csv\")\n",
    "\n",
    "if os.path.exists(unweighted_unifrac_sample_cost_path):\n",
    "    unweighted_unifrac_cost_df = pd.read_csv(unweighted_unifrac_sample_cost_path, index_col=0)    \n",
    "else:\n",
    "    # Calculating the Unweighted UniFrac cost matrix\n",
    "    COLUMNS = otus.columns\n",
    "    unweighted_unifrac_cost_matrix = np.zeros((len(COLUMNS), len(COLUMNS)))\n",
    "    taxa = otus.index.to_list()\n",
    "\n",
    "    for i, sample_i in enumerate(COLUMNS):\n",
    "        for j, sample_j in enumerate(COLUMNS):\n",
    "            if i == j:\n",
    "                unweighted_unifrac_cost_matrix[i, j] = 0\n",
    "            else:    \n",
    "                u_counts = otus[sample_i].values.copy()\n",
    "                v_counts = otus[sample_j].values.copy()\n",
    "                uuf = unweighted_unifrac(u_counts=u_counts,\n",
    "                                        v_counts=v_counts,\n",
    "                                        taxa=taxa,\n",
    "                                        tree=skbio_subtree)\n",
    "                \n",
    "                unweighted_unifrac_cost_matrix[i, j] = uuf\n",
    "\n",
    "    unweighted_unifrac_cost_df = pd.DataFrame(unweighted_unifrac_cost_matrix, index=COLUMNS, columns=COLUMNS)\n",
    "    unweighted_unifrac_cost_df.to_csv(unweighted_unifrac_sample_cost_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted UniFrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_unifrac_sample_cost_path = os.path.join(SAMPLE_DFS_DIR, \"weighted_unifrac_sample_cost_df.csv\")\n",
    "\n",
    "if os.path.exists(weighted_unifrac_sample_cost_path):\n",
    "    weighted_unifrac_cost_df = pd.read_csv(weighted_unifrac_sample_cost_path, index_col=0)    \n",
    "else:\n",
    "    # Get weighted unifrac\n",
    "    weighted_unifrac_cost_matrix = np.zeros((len(otus.columns), len(otus.columns)))\n",
    "    taxa = otus.index.to_list()\n",
    "\n",
    "    for i, sample_i in enumerate(otus.columns):\n",
    "        for j, sample_j in enumerate(otus.columns):\n",
    "            if i == j:\n",
    "                weighted_unifrac_cost_matrix[i, j] = 0\n",
    "            else:\n",
    "                u_counts = otus[sample_i].values\n",
    "                v_counts = otus[sample_j].values\n",
    "                wuf = weighted_unifrac(u_counts=u_counts,\n",
    "                                    v_counts=v_counts,\n",
    "                                    taxa=taxa,\n",
    "                                    tree=skbio_subtree)\n",
    "\n",
    "                weighted_unifrac_cost_matrix[i, j] = wuf\n",
    "\n",
    "    weighted_unifrac_cost_df = pd.DataFrame(weighted_unifrac_cost_matrix, index=otus.columns, columns=otus.columns)\n",
    "    weighted_unifrac_cost_df.to_csv(weighted_unifrac_sample_cost_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Metadata-only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_columns_continuous = [\"Age at diagnosis\"]\n",
    "\n",
    "metadata_columns_categorical = [#\"Bowel frequency during the day\",\n",
    "                                \"Soft drinks, tea or coffee with sugar (corn syrup, maple syrup, cane sugar, etc)\",\n",
    "                                \"Diet soft drinks, tea or coffee with sugar (Stevia, Equal, Splenda etc)\",\n",
    "                                \"Antibiotics\",\n",
    "                                \"Occupation\",\n",
    "                                \"Specify race\",\n",
    "                                \"sex\",\n",
    "                                \"sample\"]\n",
    "\n",
    "subset_metadata_df = ibd_metadata[metadata_columns_categorical].copy()\n",
    "subset_metadata_df.set_index(\"sample\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pandas shenanigans to get only columns where the sum of the \n",
    "# subset of OTUs from the genome dataframe sum to > 0\n",
    "# This is to prevent infeasible problems in the OT formulation\n",
    "samples = otus.T.loc[otus.loc[genome_df_columns].sum(axis=0) != 0].T.columns.to_list()\n",
    "\n",
    "# Create a one-hot encoded metadata numpy array and pandas dataframe\n",
    "enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "subset_metadata_one_hot = enc.fit_transform(subset_metadata_df.loc[samples])\n",
    "subset_metadata_one_hot_df = pd.DataFrame(subset_metadata_one_hot, \n",
    "                                          index=samples, \n",
    "                                          columns=[f\"metadata_{i}\" for i in range(subset_metadata_one_hot.shape[1])])\n",
    "\n",
    "sample_cost_dfs = {\"OT_Levenshtein\": levenshtein_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Alignment\": alignment_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Genome\": genome_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Genome_Normalized\": genome_normalized_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Phylogenetic\": phylogenetic_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"Metadata\": subset_metadata_df.loc[samples],\n",
    "                   \"Unweighted UniFrac\": unweighted_unifrac_cost_df[samples].loc[samples].copy(),\n",
    "                   \"Weighted UniFrac\": weighted_unifrac_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Levenshtein_plus_Metadata\": pd.concat([levenshtein_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Alignment_plus_Metadata\": pd.concat([alignment_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Genome_plus_Metadata\": pd.concat([genome_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Genome_Normalized_plus_Metadata\": pd.concat([genome_normalized_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Levenshtein_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(levenshtein_sample_cost_df.loc[samples].to_numpy()), index=levenshtein_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Alignment_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(alignment_sample_cost_df.loc[samples].to_numpy()), index=alignment_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Genome_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(genome_sample_cost_df.loc[samples].to_numpy()), index=genome_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Genome_Normalized_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(genome_normalized_sample_cost_df.loc[samples].to_numpy()), index=genome_normalized_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"Weighted_UniFrac_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(weighted_unifrac_cost_df.loc[samples].to_numpy()), index=weighted_unifrac_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"Unweighted_UniFrac_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(unweighted_unifrac_cost_df.loc[samples].to_numpy()), index=unweighted_unifrac_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "}\n",
    "\n",
    "METHODS_SHAPE = len(sample_cost_dfs.keys())\n",
    "all_classifiers = [\"random_forest\", \"decision_tree\", \"knn\", \"svc\"]\n",
    "score_types = [\"f1\", \"accuracy\"]\n",
    "all_input_matrices = list(sample_cost_dfs.keys())\n",
    "CLASSIFIERS_SHAPE = len(all_classifiers) * len(score_types)\n",
    "RAW_SCORE_DF_SHAPE = len(all_classifiers) * len(score_types) * len(all_input_matrices)\n",
    "\n",
    "multiindex_tuples = []\n",
    "\n",
    "for _type in score_types:\n",
    "    for _classifier in all_classifiers:\n",
    "        multiindex_tuples.append((_type, _classifier))\n",
    "\n",
    "multiindex = pd.MultiIndex.from_tuples(multiindex_tuples, names=[\"score_type\", \"classifier\"])\n",
    "\n",
    "multiindex_tuples_raw = []\n",
    "\n",
    "for _type in score_types:\n",
    "    for _classifier in all_classifiers:\n",
    "        for _input_matrix in all_input_matrices:\n",
    "            multiindex_tuples_raw.append((_type, _classifier, _input_matrix))\n",
    "\n",
    "multiindex_raw = pd.MultiIndex.from_tuples(multiindex_tuples_raw, names=[\"score_type\", \"classifier\", \"input_matrix\"])\n",
    "\n",
    "empty_arr = np.zeros((CLASSIFIERS_SHAPE, METHODS_SHAPE), dtype=float)\n",
    "raw_empty_arr = np.zeros((RAW_SCORE_DF_SHAPE, N_FOLDS), dtype=float)\n",
    "raw_score_df = pd.DataFrame(raw_empty_arr,\n",
    "                            columns=[f\"{i}\" for i in range(N_FOLDS)],\n",
    "                            index=multiindex_raw,\n",
    "                            copy=True)\n",
    "\n",
    "score_df = pd.DataFrame(data=empty_arr,\n",
    "                        columns=list(sample_cost_dfs.keys()),\n",
    "                        index=multiindex,\n",
    "                        copy=True)\n",
    "\n",
    "std_deviation_df = pd.DataFrame(data=empty_arr,\n",
    "                                columns=list(sample_cost_dfs.keys()),\n",
    "                                index=multiindex,\n",
    "                                copy=True)\n",
    "\n",
    "predictions: List[pd.Series] = []\n",
    "\n",
    "def generate_input_matrix(cost_df, _type, samples):\n",
    "    if _type == \"Metadata\":\n",
    "        enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "        X = enc.fit_transform(subset_metadata_df.loc[samples])\n",
    "    else:\n",
    "        X = cost_df.to_numpy()\n",
    "    \n",
    "    return X\n",
    "\n",
    "outer_prog_bar = tqdm(sample_cost_dfs.items(), total=len(sample_cost_dfs.keys()))\n",
    "\n",
    "for name, cost_df in outer_prog_bar:\n",
    "    outer_prog_bar.set_description(desc=name)\n",
    "    \n",
    "    labels = ibd_metadata_diagnosis.loc[cost_df.index].values.ravel()\n",
    "    X = generate_input_matrix(cost_df, _type=name, samples=samples)\n",
    "    \n",
    "    non_distance_matrices = [\n",
    "        \"Metadata\",\n",
    "        \"OT_Levenshtein_plus_Metadata\",\n",
    "        \"OT_Alignment_plus_Metadata\",\n",
    "        \"OT_Genome_plus_Metadata\",\n",
    "        \"OT_Genome_Normalized_plus_Metadata\",\n",
    "        \"OT_Levenshtein_PCA_6\",\n",
    "        \"OT_Alignment_PCA_6\",\n",
    "        \"OT_Genome_PCA_6\",\n",
    "        \"OT_Genome_Normalized_PCA_6\",\n",
    "        \"Weighted_UniFrac_PCA_6\",\n",
    "        \"Unweighted_UniFrac_PCA_6\",\n",
    "    ]\n",
    "    \n",
    "    classifiers = {\"random_forest\": RandomForestClassifier(random_state=42),\n",
    "                   \"decision_tree\": DecisionTreeClassifier(random_state=42),\n",
    "                   \"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "                   \"svc\": SVC(kernel='linear', max_iter=1000000, class_weight='balanced') if name in non_distance_matrices else SVC(kernel='precomputed', max_iter=1000000, class_weight='balanced')}\n",
    "    \n",
    "    for cl_name, classifier in classifiers.items():\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "        cv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "        # Keep track of predictions\n",
    "        y_pred = cross_val_predict(classifier, X, y, cv=cv)\n",
    "        prediction_series = pd.Series(y_pred, index=labels, name=f\"{name}_{cl_name}\")\n",
    "        predictions.append(prediction_series)\n",
    "        \n",
    "        f1 = cross_val_score(classifier, X, y, cv=cv, scoring='f1_weighted')\n",
    "        acc = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        raw_score_df.loc[(\"accuracy\", cl_name, name)] = acc\n",
    "        raw_score_df.loc[(\"f1\", cl_name, name)] = f1\n",
    "        \n",
    "        score_df.at[(\"accuracy\", cl_name), name] = acc.mean().item()\n",
    "        score_df.at[(\"f1\", cl_name), name] = f1.mean().item()\n",
    "        \n",
    "        std_deviation_df.at[(\"accuracy\", cl_name), name] = acc.std().item()\n",
    "        std_deviation_df.at[(\"f1\", cl_name), name] = f1.std().item()\n",
    "\n",
    "predictions.append(pd.Series(y, index=labels, name=\"Original\"))\n",
    "predictions_df = pd.concat(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each metric (used feature in Cursor to generate unique colors)\n",
    "# TODO: Create better way to do this\n",
    "colors = {\n",
    "    \"OT_Levenshtein\": '#1f77b4',\n",
    "    \"OT_Alignment\": '#ff7f0e',\n",
    "    \"OT_Genome\": '#2ca02c', \n",
    "    \"OT_Genome_Normalized\": '#d62728',\n",
    "    \"OT_Phylogenetic\": '#9467bd',\n",
    "    \"Metadata\": '#9467bd',\n",
    "    \"Unweighted UniFrac\": '#8c564b',\n",
    "    \"Weighted UniFrac\": '#e377c2',\n",
    "    \"OT_Levenshtein_plus_Metadata\": '#7f7f7f',\n",
    "    \"OT_Alignment_plus_Metadata\": '#bcbd22',\n",
    "    \"OT_Genome_plus_Metadata\": '#17becf',\n",
    "    \"OT_Genome_Normalized_plus_Metadata\": '#aec7e8',\n",
    "    \"OT_Levenshtein_PCA_6\": '#ff9896',\n",
    "    \"OT_Alignment_PCA_6\": '#98df8a', \n",
    "    \"OT_Genome_PCA_6\": '#c5b0d5',\n",
    "    \"OT_Genome_Normalized_PCA_6\": '#c49c94',\n",
    "    \"Weighted_UniFrac_PCA_6\": '#f7b6d2',\n",
    "    \"Unweighted_UniFrac_PCA_6\": '#dbdb8d'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 24))\n",
    "\n",
    "for idx, classifier in enumerate([\"random_forest\", \"decision_tree\", \"knn\", \"svc\"]):\n",
    "    for score_idx, score_type in enumerate([\"f1\", \"accuracy\"]):\n",
    "        data = score_df.loc[score_type, classifier].sort_values().copy()\n",
    "        errors = std_deviation_df.loc[score_type, classifier][data.index]\n",
    "        \n",
    "        axes[idx, score_idx].barh(data.index,\n",
    "                                 data.values,\n",
    "                                 color=[colors[x] for x in data.index],\n",
    "                                 xerr=errors,\n",
    "                                 error_kw={'marker': \"x\", \"capsize\": 4, \"markeredgecolor\": \"black\", \"markerfacecolor\": \"black\", \"markersize\": 4})\n",
    "        axes[idx, score_idx].set_xlim(0, 1)\n",
    "        axes[idx, score_idx].set_title(f\"{score_type.capitalize()} - {classifier.replace('_', ' ').upper()}\")\n",
    "\n",
    "fig.suptitle(\"All Scores Not Stratified by Sex\", y=1.01, fontsize=16)\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(\"figures\", \"all_scores\"), format=\"pdf\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Performance per Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframes\n",
    "cd_df = predictions_df.loc[\"CD\"].sort_values(by=predictions_df.columns.to_list(), axis=0, ascending=False)\n",
    "uc_df = predictions_df.loc[\"UC\"].sort_values(by=predictions_df.columns.to_list(), axis=0, ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 20))\n",
    "\n",
    "# Common kwargs for both heatmaps\n",
    "kwargs = {\n",
    "    'cmap': 'binary',\n",
    "    'linewidths': 0.5,\n",
    "    'linecolor': 'black',\n",
    "    'xticklabels': True,\n",
    "    'yticklabels': False,\n",
    "    'annot': True,\n",
    "    'cbar': False\n",
    "}\n",
    "\n",
    "# CD\n",
    "original_column_index_cd = len(cd_df.columns) - 1\n",
    "cd_df_sorted_columnwise = cd_df[cd_df.sum().sort_values(ascending=False).index].copy()\n",
    "perfect_classification_columns_cd = [col for col in cd_df_sorted_columnwise.columns if (cd_df_sorted_columnwise[col] == 0).all()]\n",
    "\n",
    "sns.heatmap(cd_df_sorted_columnwise, ax=ax[0], **kwargs)\n",
    "ax[0].axvline(x=original_column_index_cd, color='red', linewidth=2, linestyle='--')\n",
    "ax[0].set_title(f\"CD Classifications on {cd_df.shape[0]} Samples\", fontsize=12)\n",
    "\n",
    "# UC\n",
    "original_column_inde_uc = len(uc_df.columns) - 1\n",
    "uc_df_sorted_columnwise = uc_df[uc_df.sum().sort_values(ascending=True).index].copy()\n",
    "perfect_classification_columns_uc = [col for col in uc_df_sorted_columnwise.columns if (uc_df_sorted_columnwise[col] == 1).all()]\n",
    "\n",
    "sns.heatmap(uc_df_sorted_columnwise, ax=ax[1], **kwargs)\n",
    "ax[1].axvline(x=original_column_inde_uc, color='red', linewidth=2, linestyle='--')\n",
    "ax[1].set_title(f\"UC Classifications on {uc_df.shape[0]} Samples\", fontsize=12)\n",
    "\n",
    "# Color perfect classification column labels blue\n",
    "for tick, label in enumerate(ax[0].get_xticklabels()):\n",
    "    txt = label.get_text()\n",
    "    \n",
    "    if txt == \"Original\":\n",
    "        continue\n",
    "    \n",
    "    if txt in perfect_classification_columns_cd:\n",
    "        label.set_color('blue')\n",
    "    elif txt in perfect_classification_columns_uc:\n",
    "        label.set_color('red')\n",
    "    elif (txt in perfect_classification_columns_uc) and (txt in perfect_classification_columns_cd):\n",
    "        label.set_color('purple')\n",
    "    else:\n",
    "        label.set_color('black')\n",
    "\n",
    "\n",
    "# Color perfect classification column labels blue\n",
    "for tick, label in enumerate(ax[1].get_xticklabels()):\n",
    "    txt = label.get_text()\n",
    "    \n",
    "    if txt == \"Original\":\n",
    "        continue\n",
    "    \n",
    "    if txt in perfect_classification_columns_cd:\n",
    "        label.set_color('blue')\n",
    "    elif txt in perfect_classification_columns_uc:\n",
    "        label.set_color('red')\n",
    "    elif (txt in perfect_classification_columns_uc) and (txt in perfect_classification_columns_cd):\n",
    "        label.set_color('purple')\n",
    "    else:\n",
    "        label.set_color('black')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Metadata Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "# X = enc.fit_transform(subset_metadata_df)\n",
    "\n",
    "# plt.figure(figsize=(24, 10))\n",
    "\n",
    "# dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# X = generate_input_matrix(sample_cost_dfs[\"Metadata\"], \"Metadata\", samples)\n",
    "# labels = ibd_metadata_diagnosis.loc[sample_cost_dfs[\"Metadata\"].index].values.ravel()\n",
    "# le = LabelEncoder()\n",
    "# y = le.fit_transform(labels)\n",
    "\n",
    "# full_feature_names = enc.get_feature_names_out(subset_metadata_df.columns)\n",
    "# truncated_feature_names = [f\"{name[:8]}...{name[-20:]}\" for name in full_feature_names]\n",
    "\n",
    "# dtree.fit(X, y)\n",
    "# plot_tree(dtree, \n",
    "#           fontsize=8, \n",
    "#           class_names=list(le.classes_),\n",
    "#           feature_names=truncated_feature_names,\n",
    "#           label='all',\n",
    "#           filled=True)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Intra vs Inter Class Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some pandas shenanigans to get only columns where the sum of the \n",
    "# subset of OTUs from the genome dataframe sum to > 0\n",
    "# This is to prevent infeasible problems in the OT formulation\n",
    "samples = otus.T.loc[otus.loc[genome_df_columns].sum(axis=0) != 0].T.columns.to_list()\n",
    "\n",
    "cd_indices = list(set(ibd_metadata_diagnosis[ibd_metadata_diagnosis[\"diagnosis\"] == \"CD\"].index) & set(samples))\n",
    "uc_indices = list(set(ibd_metadata_diagnosis[ibd_metadata_diagnosis[\"diagnosis\"] == \"UC\"].index) & set(samples))\n",
    "\n",
    "dfs = {\n",
    "    'OT_Levenshtein': levenshtein_sample_cost_df,\n",
    "    'OT_Alignment': alignment_sample_cost_df,\n",
    "    \"OT_Phylogenetic\": phylogenetic_sample_cost_df,\n",
    "    'Weighted UniFrac': weighted_unifrac_cost_df,\n",
    "    'Unweighted UniFrac': unweighted_unifrac_cost_df,\n",
    "    'OT_Genome_Normalized': genome_normalized_sample_cost_df,\n",
    "    'OT_Genome': genome_sample_cost_df,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(8, 1, figsize=(10, 10), sharex=False, sharey=False)\n",
    "\n",
    "for idx, (df_name, df) in enumerate(dfs.items()):\n",
    "    intra_cd = df.loc[cd_indices][cd_indices].to_numpy().flatten()\n",
    "    intra_uc = df.loc[uc_indices][uc_indices].to_numpy().flatten()\n",
    "    inter_1 = df.loc[cd_indices][uc_indices].to_numpy().flatten()\n",
    "    inter_2 = df.loc[uc_indices][cd_indices].to_numpy().flatten()\n",
    "\n",
    "    sns.kdeplot(data=intra_cd, label=f\"Intra Class Distance (CD)\", ax=ax[idx], alpha=0.5, fill=True, warn_singular=False)\n",
    "    sns.kdeplot(data=intra_uc, label=f\"Intra Class Distance (UC)\", ax=ax[idx], alpha=0.5, fill=True, warn_singular=False)\n",
    "    sns.kdeplot(data=inter_1, label=f\"Inter Class Distance\", ax=ax[idx], alpha=0.5, fill=True, warn_singular=False)\n",
    "    \n",
    "    ax[idx].set_title(df_name)\n",
    "    ax[idx].legend()\n",
    "\n",
    "otus_l2_distances_cd = distance_matrix(otus[cd_indices].values, otus[cd_indices].values, p=2).flatten()\n",
    "otus_l2_distances_uc = distance_matrix(otus[uc_indices].values, otus[uc_indices].values, p=2).flatten()\n",
    "# otus_l2_distances = pd.DataFrame(distance_matrix(otus.values, otus.values), index=otus.columns.to_list())\n",
    "\n",
    "sns.kdeplot(data=otus_l2_distances_cd, label=\"Intra Class L2 Distances (CD)\", ax=ax[7], alpha=0.5, fill=True)\n",
    "sns.kdeplot(data=otus_l2_distances_uc, label=\"Intra Class L2 Distances (UC)\", ax=ax[7], alpha=0.5, fill=True)\n",
    "# sns.kdeplot(data=otus_l2_distances_inter, label=\"Inter Class L2 Distances\", ax=ax[6], alpha=0.5, fill=True)\n",
    "ax[7].set_title(\"L2 Distance between Pairwise Normalized OTU Counts\")\n",
    "ax[7].legend()\n",
    "\n",
    "# fig.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig(\"figures/intra_vs_inter_class_distances.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratifying by Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_samples = set(ibd_metadata[ibd_metadata[\"sex\"] == \"Male\"][\"sample\"].to_list())\n",
    "female_samples = set(ibd_metadata[ibd_metadata[\"sex\"] == \"Female\"][\"sample\"].to_list())\n",
    "\n",
    "# Some pandas shenanigans to get only columns where the sum of the \n",
    "# subset of OTUs from the genome dataframe sum to > 0\n",
    "# This is to prevent infeasible problems in the OT formulation\n",
    "valid_genome_samples = set(otus.T.loc[otus.loc[genome_df_columns].sum(axis=0) != 0].T.columns.to_list())\n",
    "\n",
    "male_samples_set = set(ibd_metadata[ibd_metadata[\"sex\"] == \"Male\"][\"sample\"].to_list())\n",
    "female_samples_set = set(ibd_metadata[ibd_metadata[\"sex\"] == \"Female\"][\"sample\"].to_list())\n",
    "\n",
    "# Some pandas shenanigans to get only columns where the sum of the \n",
    "# subset of OTUs from the genome dataframe sum to > 0\n",
    "# This is to prevent infeasible problems in the OT formulation\n",
    "valid_genome_samples_set = set(otus.T.loc[otus.loc[genome_df_columns].sum(axis=0) != 0].T.columns.to_list())\n",
    "valid_genome_samples_list = otus.T.loc[otus.loc[genome_df_columns].sum(axis=0) != 0].T.columns.to_list()\n",
    "\n",
    "male_samples_list = list(male_samples_set & valid_genome_samples_set)\n",
    "female_samples_list = list(female_samples_set & valid_genome_samples_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {\n",
    "    \"levenshtein\": levenshtein_sample_cost_df.copy(),\n",
    "    \"alignment\": alignment_sample_cost_df.copy(),\n",
    "    \"genome\": genome_sample_cost_df.copy(),\n",
    "    \"genome_normalized\": genome_normalized_sample_cost_df.copy()\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(3, len(dfs.values()), figsize=(14, 9))\n",
    "\n",
    "for idx, (name, df) in enumerate(dfs.items()):\n",
    "    all_samples = PCA(n_components=6).fit_transform(df[valid_genome_samples_list].loc[valid_genome_samples_list].to_numpy())\n",
    "    male_samples = PCA(n_components=6).fit_transform(df[male_samples_list].loc[male_samples_list].to_numpy())\n",
    "    female_samples = PCA(n_components=6).fit_transform(df[female_samples_list].loc[female_samples_list].to_numpy())\n",
    "\n",
    "    ax[0][idx].scatter(all_samples[:, 0], \n",
    "                all_samples[:, 1], \n",
    "                color=ibd_metadata_diagnosis.loc[valid_genome_samples_list][\"diagnosis\"].apply(lambda x: \"red\" if x == \"CD\" else \"blue\").values)\n",
    "\n",
    "    ax[1][idx].scatter(male_samples[:, 0], \n",
    "                male_samples[:, 1], \n",
    "                color=ibd_metadata_diagnosis.loc[male_samples_list][\"diagnosis\"].apply(lambda x: \"red\" if x == \"CD\" else \"blue\").values)\n",
    "\n",
    "    ax[2][idx].scatter(female_samples[:, 0], \n",
    "                female_samples[:, 1], \n",
    "                color=ibd_metadata_diagnosis.loc[female_samples_list][\"diagnosis\"].apply(lambda x: \"red\" if x == \"CD\" else \"blue\").values)\n",
    "    \n",
    "    if idx == 0:\n",
    "        ax[0][idx].text(-0.6, 0.5, \"Male + Female\", rotation=90, transform=ax[0][idx].transAxes, va='center')\n",
    "        ax[1][idx].text(-0.6, 0.5, \"Male\", rotation=90, transform=ax[1][idx].transAxes, va='center')\n",
    "        ax[2][idx].text(-0.6, 0.5, \"Female\", rotation=90, transform=ax[2][idx].transAxes, va='center')\n",
    "    \n",
    "        ax[0][idx].set_ylabel(\"PCA 2\")\n",
    "        ax[1][idx].set_ylabel(\"PCA 2\") \n",
    "        ax[2][idx].set_ylabel(\"PCA 2\")\n",
    "    \n",
    "    ax[0][idx].set_title(name)\n",
    "    \n",
    "    # Add legend to bottom subplots\n",
    "    if idx == len(dfs.items()) - 1:\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', label='CD', markersize=10),\n",
    "                         plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', label='UC', markersize=10)]\n",
    "        ax[2][idx].legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "fig.suptitle(\"6-component PCA by OT Cost Matrix Stratified by Sex\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(male_samples_set & valid_genome_samples_set)\n",
    "\n",
    "# Create a one-hot encoded metadata numpy array and pandas dataframe\n",
    "enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "subset_metadata_one_hot = enc.fit_transform(subset_metadata_df.loc[samples])\n",
    "subset_metadata_one_hot_df = pd.DataFrame(subset_metadata_one_hot, \n",
    "                                          index=samples, \n",
    "                                          columns=[f\"metadata_{i}\" for i in range(subset_metadata_one_hot.shape[1])])\n",
    "\n",
    "sample_cost_dfs = {\"OT_Levenshtein\": levenshtein_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Alignment\": alignment_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Genome\": genome_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Genome_Normalized\": genome_normalized_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"Metadata\": subset_metadata_df.loc[samples],\n",
    "                   \"Unweighted UniFrac\": unweighted_unifrac_cost_df[samples].loc[samples].copy(),\n",
    "                   \"Weighted UniFrac\": weighted_unifrac_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Levenshtein_plus_Metadata\": pd.concat([levenshtein_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Alignment_plus_Metadata\": pd.concat([alignment_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Genome_plus_Metadata\": pd.concat([genome_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Genome_Normalized_plus_Metadata\": pd.concat([genome_normalized_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Levenshtein_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(levenshtein_sample_cost_df.loc[samples].to_numpy()), index=levenshtein_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Alignment_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(alignment_sample_cost_df.loc[samples].to_numpy()), index=alignment_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Genome_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(genome_sample_cost_df.loc[samples].to_numpy()), index=genome_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Genome_Normalized_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(genome_normalized_sample_cost_df.loc[samples].to_numpy()), index=genome_normalized_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"Weighted_UniFrac_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(weighted_unifrac_cost_df.loc[samples].to_numpy()), index=weighted_unifrac_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"Unweighted_UniFrac_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(unweighted_unifrac_cost_df.loc[samples].to_numpy()), index=unweighted_unifrac_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "}\n",
    "\n",
    "METHODS_SHAPE = len(sample_cost_dfs.keys())\n",
    "all_classifiers = [\"random_forest\", \"decision_tree\", \"knn\", \"svc\"]\n",
    "score_types = [\"f1\", \"accuracy\"]\n",
    "CLASSIFIERS_SHAPE = len(all_classifiers) * len(score_types)\n",
    "\n",
    "multiindex_tuples = []\n",
    "\n",
    "for _type in score_types:\n",
    "    for _classifier in all_classifiers:\n",
    "        multiindex_tuples.append((_type, _classifier))\n",
    "\n",
    "multiindex = pd.MultiIndex.from_tuples(multiindex_tuples, names=[\"score_type\", \"classifier\"])\n",
    "\n",
    "empty_arr = np.zeros((CLASSIFIERS_SHAPE, METHODS_SHAPE), dtype=float)\n",
    "score_df = pd.DataFrame(data=empty_arr,\n",
    "                        columns=list(sample_cost_dfs.keys()),\n",
    "                        index=multiindex,\n",
    "                        copy=True)\n",
    "\n",
    "std_deviation_df = pd.DataFrame(data=empty_arr,\n",
    "                                columns=list(sample_cost_dfs.keys()),\n",
    "                                index=multiindex,\n",
    "                                copy=True)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "def generate_input_matrix(cost_df, _type, samples):\n",
    "    if _type == \"Metadata\":\n",
    "        enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "        X = enc.fit_transform(subset_metadata_df.loc[samples])\n",
    "    else:\n",
    "        X = cost_df.to_numpy()\n",
    "    \n",
    "    return X\n",
    "\n",
    "outer_prog_bar = tqdm(sample_cost_dfs.items(), total=len(sample_cost_dfs.keys()))\n",
    "\n",
    "for name, cost_df in outer_prog_bar:\n",
    "    outer_prog_bar.set_description(desc=name)\n",
    "    \n",
    "    labels = ibd_metadata_diagnosis.loc[cost_df.index].values.ravel()\n",
    "    X = generate_input_matrix(cost_df, _type=name, samples=samples)\n",
    "    \n",
    "    non_distance_matrices = [\n",
    "        \"Metadata\",\n",
    "        \"OT_Levenshtein_plus_Metadata\",\n",
    "        \"OT_Alignment_plus_Metadata\",\n",
    "        \"OT_Genome_plus_Metadata\",\n",
    "        \"OT_Genome_Normalized_plus_Metadata\",\n",
    "        \"OT_Levenshtein_PCA_6\",\n",
    "        \"OT_Alignment_PCA_6\",\n",
    "        \"OT_Genome_PCA_6\",\n",
    "        \"OT_Genome_Normalized_PCA_6\",\n",
    "        \"Weighted_UniFrac_PCA_6\",\n",
    "        \"Unweighted_UniFrac_PCA_6\",\n",
    "    ]\n",
    "    \n",
    "    classifiers = {\"random_forest\": RandomForestClassifier(random_state=42),\n",
    "                   \"decision_tree\": DecisionTreeClassifier(random_state=42),\n",
    "                   \"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "                   \"svc\": SVC(kernel='linear', max_iter=1000000) if name in non_distance_matrices else SVC(kernel='precomputed', max_iter=1000000)}\n",
    "    \n",
    "    for cl_name, classifier in classifiers.items():\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "        cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "        # Keep track of predictions\n",
    "        y_pred = cross_val_predict(classifier, X, y, cv=cv)\n",
    "        prediction_series = pd.Series(y_pred, index=labels, name=f\"{name}_{cl_name}\")\n",
    "        predictions.append(prediction_series)\n",
    "        \n",
    "        f1 = cross_val_score(classifier, X, y, cv=cv, scoring='f1_weighted')\n",
    "        acc = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "        score_df.at[(\"accuracy\", cl_name), name] = acc.mean().item()\n",
    "        score_df.at[(\"f1\", cl_name), name] = f1.mean().item()\n",
    "        \n",
    "        std_deviation_df.at[(\"accuracy\", cl_name), name] = acc.std().item()\n",
    "        std_deviation_df.at[(\"f1\", cl_name), name] = f1.std().item()\n",
    "\n",
    "predictions.append(pd.Series(y, index=labels, name=\"Original\"))\n",
    "predictions_df = pd.concat(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each metric (used feature in Cursor to generate unique colors)\n",
    "# TODO: Create better way to do this\n",
    "colors = {\n",
    "    \"OT_Levenshtein\": '#1f77b4',\n",
    "    \"OT_Alignment\": '#ff7f0e',\n",
    "    \"OT_Genome\": '#2ca02c', \n",
    "    \"OT_Genome_Normalized\": '#d62728',\n",
    "    \"OT_Phylogenetic\": '#9467bd',\n",
    "    \"Metadata\": '#9467bd',\n",
    "    \"Unweighted UniFrac\": '#8c564b',\n",
    "    \"Weighted UniFrac\": '#e377c2',\n",
    "    \"OT_Levenshtein_plus_Metadata\": '#7f7f7f',\n",
    "    \"OT_Alignment_plus_Metadata\": '#bcbd22',\n",
    "    \"OT_Genome_plus_Metadata\": '#17becf',\n",
    "    \"OT_Genome_Normalized_plus_Metadata\": '#aec7e8',\n",
    "    \"OT_Levenshtein_PCA_6\": '#ff9896',\n",
    "    \"OT_Alignment_PCA_6\": '#98df8a', \n",
    "    \"OT_Genome_PCA_6\": '#c5b0d5',\n",
    "    \"OT_Genome_Normalized_PCA_6\": '#c49c94',\n",
    "    \"Weighted_UniFrac_PCA_6\": '#f7b6d2',\n",
    "    \"Unweighted_UniFrac_PCA_6\": '#dbdb8d'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 24))\n",
    "\n",
    "for idx, classifier in enumerate([\"random_forest\", \"decision_tree\", \"knn\", \"svc\"]):\n",
    "    for score_idx, score_type in enumerate([\"f1\", \"accuracy\"]):\n",
    "        data = score_df.loc[score_type, classifier].sort_values().copy()\n",
    "        errors = std_deviation_df.loc[score_type, classifier][data.index]\n",
    "        \n",
    "        axes[idx, score_idx].barh(data.index,\n",
    "                                 data.values,\n",
    "                                 color=[colors[x] for x in data.index],\n",
    "                                 xerr=errors,\n",
    "                                 error_kw={'marker': \"x\", \"capsize\": 4, \"markeredgecolor\": \"black\", \"markerfacecolor\": \"black\", \"markersize\": 4})\n",
    "        axes[idx, score_idx].set_xlim(0, 1)\n",
    "        axes[idx, score_idx].set_title(f\"{score_type.capitalize()} - {classifier.replace('_', ' ').upper()}\")\n",
    "\n",
    "fig.suptitle(\"All Scores - Males Only\", y=1.01, fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframes\n",
    "cd_df = predictions_df.loc[\"CD\"].sort_values(by=predictions_df.columns.to_list(), axis=0, ascending=False)\n",
    "uc_df = predictions_df.loc[\"UC\"].sort_values(by=predictions_df.columns.to_list(), axis=0, ascending=False)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "sns.set_theme(font_scale=0.6)\n",
    "\n",
    "# Common kwargs for both heatmaps\n",
    "kwargs = {\n",
    "    'cmap': 'binary',\n",
    "    'linewidths': 0.5,\n",
    "    'linecolor': 'black',\n",
    "    'xticklabels': True,\n",
    "    'yticklabels': True,\n",
    "    'annot': True,\n",
    "    'cbar': False\n",
    "}\n",
    "\n",
    "# Create the heatmaps with masks\n",
    "# sns.heatmap(cd_df[cd_df.any(axis=1)], ax=ax[0], **kwargs)\n",
    "# sns.heatmap(uc_df[uc_df[uc_df.columns[:-1]].any(axis=1)], ax=ax[1], **kwargs)\n",
    "\n",
    "sns.heatmap(cd_df, ax=ax[0], **kwargs)\n",
    "sns.heatmap(uc_df, ax=ax[1], **kwargs)\n",
    "\n",
    "first_metadata_col_index = sorted([i for i, col in enumerate(cd_df.columns) if col.startswith(\"Metadata\")])[0]\n",
    "first_unifrac_col_index = sorted([i for i, col in enumerate(cd_df.columns) if \"UniFrac\" in col])[0]\n",
    "original_column_index = len(cd_df.columns) - 1\n",
    "\n",
    "# Formatting\n",
    "for a in ax:\n",
    "    # Rotate labels for better readability\n",
    "    a.set_xticklabels(a.get_xticklabels(), rotation=45, ha='right')\n",
    "    a.set_yticklabels(a.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    # # Optional: Add vertical lines to further emphasize groups\n",
    "    # a.axvline(x=first_metadata_col_index, color='red', linewidth=2, linestyle='--')  # After OT methods\n",
    "    # a.axvline(x=first_unifrac_col_index, color='red', linewidth=2, linestyle='--')  # After Metadata\n",
    "    # a.axvline(x=original_column_index, color='red', linewidth=2, linestyle='--') # After UniFrac\n",
    "\n",
    "# Add titles\n",
    "ax[0].set_title('CD Predictions')\n",
    "ax[1].set_title('UC Predictions')\n",
    "fig.suptitle(\"Predictions vs Original Data by Label - Males Only\")\n",
    "\n",
    "# Adjust layout to prevent label cutoff\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(female_samples_set & valid_genome_samples_set)\n",
    "\n",
    "# Create a one-hot encoded metadata numpy array and pandas dataframe\n",
    "enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "subset_metadata_one_hot = enc.fit_transform(subset_metadata_df.loc[samples])\n",
    "subset_metadata_one_hot_df = pd.DataFrame(subset_metadata_one_hot, \n",
    "                                          index=samples, \n",
    "                                          columns=[f\"metadata_{i}\" for i in range(subset_metadata_one_hot.shape[1])])\n",
    "\n",
    "sample_cost_dfs = {\"OT_Levenshtein\": levenshtein_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Alignment\": alignment_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Genome\": genome_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Genome_Normalized\": genome_normalized_sample_cost_df[samples].loc[samples].copy(),\n",
    "                   \"Metadata\": subset_metadata_df.loc[samples],\n",
    "                   \"Unweighted UniFrac\": unweighted_unifrac_cost_df[samples].loc[samples].copy(),\n",
    "                   \"Weighted UniFrac\": weighted_unifrac_cost_df[samples].loc[samples].copy(),\n",
    "                   \"OT_Levenshtein_plus_Metadata\": pd.concat([levenshtein_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Alignment_plus_Metadata\": pd.concat([alignment_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Genome_plus_Metadata\": pd.concat([genome_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Genome_Normalized_plus_Metadata\": pd.concat([genome_normalized_sample_cost_df[samples].loc[samples], subset_metadata_one_hot_df], join=\"inner\", axis=1),\n",
    "                   \"OT_Levenshtein_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(levenshtein_sample_cost_df.loc[samples].to_numpy()), index=levenshtein_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Alignment_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(alignment_sample_cost_df.loc[samples].to_numpy()), index=alignment_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Genome_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(genome_sample_cost_df.loc[samples].to_numpy()), index=genome_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"OT_Genome_Normalized_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(genome_normalized_sample_cost_df.loc[samples].to_numpy()), index=genome_normalized_sample_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"Weighted_UniFrac_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(weighted_unifrac_cost_df.loc[samples].to_numpy()), index=weighted_unifrac_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "                   \"Unweighted_UniFrac_PCA_6\": pd.DataFrame(PCA(n_components=6).fit_transform(unweighted_unifrac_cost_df.loc[samples].to_numpy()), index=unweighted_unifrac_cost_df.loc[samples].index, columns=[f\"component_{i+1}\" for i in range(6)]),\n",
    "}\n",
    "\n",
    "METHODS_SHAPE = len(sample_cost_dfs.keys())\n",
    "all_classifiers = [\"random_forest\", \"decision_tree\", \"knn\", \"svc\"]\n",
    "score_types = [\"f1\", \"accuracy\"]\n",
    "CLASSIFIERS_SHAPE = len(all_classifiers) * len(score_types)\n",
    "\n",
    "multiindex_tuples = []\n",
    "\n",
    "for _type in score_types:\n",
    "    for _classifier in all_classifiers:\n",
    "        multiindex_tuples.append((_type, _classifier))\n",
    "\n",
    "multiindex = pd.MultiIndex.from_tuples(multiindex_tuples, names=[\"score_type\", \"classifier\"])\n",
    "\n",
    "empty_arr = np.zeros((CLASSIFIERS_SHAPE, METHODS_SHAPE), dtype=float)\n",
    "score_df = pd.DataFrame(data=empty_arr,\n",
    "                        columns=list(sample_cost_dfs.keys()),\n",
    "                        index=multiindex,\n",
    "                        copy=True)\n",
    "\n",
    "std_deviation_df = pd.DataFrame(data=empty_arr,\n",
    "                                columns=list(sample_cost_dfs.keys()),\n",
    "                                index=multiindex,\n",
    "                                copy=True)\n",
    "\n",
    "predictions = []\n",
    "\n",
    "def generate_input_matrix(cost_df, _type, samples):\n",
    "    if _type == \"Metadata\":\n",
    "        enc = OneHotEncoder(categories='auto', sparse_output=False)\n",
    "        X = enc.fit_transform(subset_metadata_df.loc[samples])\n",
    "    else:\n",
    "        X = cost_df.to_numpy()\n",
    "    \n",
    "    return X\n",
    "\n",
    "outer_prog_bar = tqdm(sample_cost_dfs.items(), total=len(sample_cost_dfs.keys()))\n",
    "\n",
    "for name, cost_df in outer_prog_bar:\n",
    "    outer_prog_bar.set_description(desc=name)\n",
    "    \n",
    "    labels = ibd_metadata_diagnosis.loc[cost_df.index].values.ravel()\n",
    "    X = generate_input_matrix(cost_df, _type=name, samples=samples)\n",
    "    \n",
    "    non_distance_matrices = [\n",
    "        \"Metadata\",\n",
    "        \"OT_Levenshtein_plus_Metadata\",\n",
    "        \"OT_Alignment_plus_Metadata\",\n",
    "        \"OT_Genome_plus_Metadata\",\n",
    "        \"OT_Genome_Normalized_plus_Metadata\",\n",
    "        \"OT_Levenshtein_PCA_6\",\n",
    "        \"OT_Alignment_PCA_6\",\n",
    "        \"OT_Genome_PCA_6\",\n",
    "        \"OT_Genome_Normalized_PCA_6\",\n",
    "        \"Weighted_UniFrac_PCA_6\",\n",
    "        \"Unweighted_UniFrac_PCA_6\",\n",
    "    ]\n",
    "    \n",
    "    classifiers = {\"random_forest\": RandomForestClassifier(random_state=42),\n",
    "                   \"decision_tree\": DecisionTreeClassifier(random_state=42),\n",
    "                   \"knn\": KNeighborsClassifier(n_neighbors=5),\n",
    "                   \"svc\": SVC(kernel='linear', max_iter=1000000) if name in non_distance_matrices else SVC(kernel='precomputed', max_iter=1000000)}\n",
    "    \n",
    "    for cl_name, classifier in classifiers.items():\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "        cv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "        # Keep track of predictions\n",
    "        y_pred = cross_val_predict(classifier, X, y, cv=cv)\n",
    "        prediction_series = pd.Series(y_pred, index=labels, name=f\"{name}_{cl_name}\")\n",
    "        predictions.append(prediction_series)\n",
    "        \n",
    "        f1 = cross_val_score(classifier, X, y, cv=cv, scoring='f1_weighted')\n",
    "        acc = cross_val_score(classifier, X, y, cv=cv, scoring='accuracy')\n",
    "        score_df.at[(\"accuracy\", cl_name), name] = acc.mean().item()\n",
    "        score_df.at[(\"f1\", cl_name), name] = f1.mean().item()\n",
    "        \n",
    "        std_deviation_df.at[(\"accuracy\", cl_name), name] = acc.std().item()\n",
    "        std_deviation_df.at[(\"f1\", cl_name), name] = f1.std().item()\n",
    "\n",
    "predictions.append(pd.Series(y, index=labels, name=\"Original\"))\n",
    "predictions_df = pd.concat(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each metric (used feature in Cursor to generate unique colors)\n",
    "# TODO: Create better way to do this\n",
    "colors = {\n",
    "    \"OT_Levenshtein\": '#1f77b4',\n",
    "    \"OT_Alignment\": '#ff7f0e',\n",
    "    \"OT_Genome\": '#2ca02c', \n",
    "    \"OT_Genome_Normalized\": '#d62728',\n",
    "    \"OT_Phylogenetic\": '#9467bd',\n",
    "    \"Metadata\": '#9467bd',\n",
    "    \"Unweighted UniFrac\": '#8c564b',\n",
    "    \"Weighted UniFrac\": '#e377c2',\n",
    "    \"OT_Levenshtein_plus_Metadata\": '#7f7f7f',\n",
    "    \"OT_Alignment_plus_Metadata\": '#bcbd22',\n",
    "    \"OT_Genome_plus_Metadata\": '#17becf',\n",
    "    \"OT_Genome_Normalized_plus_Metadata\": '#aec7e8',\n",
    "    \"OT_Levenshtein_PCA_6\": '#ff9896',\n",
    "    \"OT_Alignment_PCA_6\": '#98df8a', \n",
    "    \"OT_Genome_PCA_6\": '#c5b0d5',\n",
    "    \"OT_Genome_Normalized_PCA_6\": '#c49c94',\n",
    "    \"Weighted_UniFrac_PCA_6\": '#f7b6d2',\n",
    "    \"Unweighted_UniFrac_PCA_6\": '#dbdb8d'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, figsize=(16, 24))\n",
    "\n",
    "for idx, classifier in enumerate([\"random_forest\", \"decision_tree\", \"knn\", \"svc\"]):\n",
    "    for score_idx, score_type in enumerate([\"f1\", \"accuracy\"]):\n",
    "        data = score_df.loc[score_type, classifier].sort_values().copy()\n",
    "        errors = std_deviation_df.loc[score_type, classifier][data.index]\n",
    "        \n",
    "        axes[idx, score_idx].barh(data.index,\n",
    "                                 data.values,\n",
    "                                 color=[colors[x] for x in data.index],\n",
    "                                 xerr=errors,\n",
    "                                 error_kw={'marker': \"x\", \"capsize\": 4, \"markeredgecolor\": \"black\", \"markerfacecolor\": \"black\", \"markersize\": 4})\n",
    "        axes[idx, score_idx].set_xlim(0, 1)\n",
    "        axes[idx, score_idx].set_title(f\"{score_type.capitalize()} - {classifier.replace('_', ' ').upper()}\")\n",
    "\n",
    "fig.suptitle(\"All Scores - Females Only\", y=1.01, fontsize=16)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beta_diversity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
